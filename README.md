# Important papers

1. Kearns M J, Vazirani U. **An introduction to computational learning theory**[M]. MIT press, 1994. [book](https://books.google.com/books?hl=zh-CN&lr=&id=vCA01wY6iywC&oi=fnd&pg=PR11&ots=p3M5eXutCA&sig=IyM8SW_y0WxvrLkwNZDv6GeOSUc)
1. 周志华. **机器学习**. 清华大学出版社, 2016. [book](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm)
1. Shalev-Shwartz S, Ben-David S. **Understanding machine learning: From theory to algorithms**[M]. Cambridge university press, 2014. [paper](http://103.47.12.35/bitstream/handle/1/1069/understanding-machine-learning-theory-algorithms.pdf?sequence=1&isAllowed=y)
1. Viering T, Mey A, Loog M. **Open problem: Monotonicity of learning**[C]//Conference on Learning Theory. PMLR, 2019: 3198-3201. [paper](http://proceedings.mlr.press/v99/viering19a/viering19a.pdf)
1. Loog M, Viering T, Mey A. **Minimizers of the empirical risk and risk monotonicity**[J]. Advances in Neural Information Processing Systems, 2019, 32: 7478-7487. [paper](https://arxiv.org/pdf/1907.05476.pdf)
1. Savage L J. **The Foundations of Statistics**[M]. Courier Corporation, 2012. [book](https://books.google.com/books?hl=zh-CN&lr=&id=N_bBAgAAQBAJ&oi=fnd&pg=PA1&ots=8uoDYHkm-n&sig=A_hAwDj_75zC4jIBsXviGyicmR4)
1. Good I J. **On the principle of total evidence**[J]. The British Journal for the Philosophy of Science, 1967, 17(4): 319-321. [paper](https://www.jstor.org/stable/pdf/686773.pdf?casa_token=D7XZRowDd30AAAAA:wmGJvHj1AvbGc76It-oP3wPeKI6n1hvqYVhJQS6OPHqHAAd48Dsv2UvZ5a3I_xw5H3zrC3gqRx73DtT9PY0P_QNTTW0vX-h__Dt1WZDa8GKdHaEWv3Wkrg)
1. Grunwald P D, Halpern J Y. **When ignorance is bliss**[J]. arXiv preprint arXiv:1407.7188, 2014. [paper](https://www.researchgate.net/profile/Peter-Gruenwald/publication/221404811_When_Ignorance_is_Bliss/links/546378a60cf2c0c6aec4c031/When-Ignorance-is-Bliss.pdf)
1. Viering T, Loog M. **The Shape of Learning Curves: a Review**[J]. arXiv preprint arXiv:2103.10948, 2021. [paper](https://arxiv.org/pdf/2103.10948.pdf)
1. Viering T J, Mey A, Loog M. **Making learners (more) monotone**[C]//International Symposium on Intelligent Data Analysis. Springer, Cham, 2020: 535-547. [paper](https://www.researchgate.net/profile/Tom-Viering/publication/337531868_Making_Learners_More_Monotone/links/5f4e07b9a6fdcc14c504d66c/Making-Learners-More-Monotone.pdf)
1. Mhammedi Z, Husain H. **Risk-Monotonicity in Statistical Learning**[J]. arXiv preprint arXiv:2011.14126, 2020. [paper](https://arxiv.org/pdf/2011.14126.pdf)
1. Soviany P, Ionescu R T, Rota P, et al. **Curriculum learning: A survey**[J]. arXiv preprint arXiv:2101.10382, 2021. [paper](https://arxiv.org/pdf/2101.10382.pdf)
1. Bengio Y, Louradour J, Collobert R, et al. **Curriculum learning**[C]//Proceedings of the 26th annual international conference on machine learning. 2009: 41-48. [paper](http://www.thespermwhale.com/jaseweston/papers/curriculum.pdf)
1. Kumar M P, Packer B, Koller D. **Self-Paced Learning for Latent Variable Models**[C]//NIPS. 2010, 1: 2. [paper](http://papers.neurips.cc/paper/3923-self-paced-learning-for-latent-variable-models.pdf)
1. Jiang L, Meng D, Zhao Q, et al. **Self-paced curriculum learning**[C]//Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015. [paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/%0B9750/9929)
1. Kim T H, Choi J. Screenernet: **Learning self-paced curriculum for deep neural networks**[J]. arXiv preprint arXiv:1801.00904, 2018. [paper](https://arxiv.org/pdf/1801.00904.pdf)
1. Morerio P, Cavazza J, Volpi R, et al. **Curriculum dropout**[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 3544-3552. [paper](http://www.vision.jhu.edu/assets/MorerioICCV17.pdf)
1. Krogh A, Vedelsby J. **Neural network ensembles, cross validation, and active learning**[J]. Advances in neural information processing systems, 1995, 7: 231-238. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9672&rep=rep1&type=pdf)
1. Gong M, Li H, Meng D, et al. **Decomposition-based evolutionary multiobjective optimization to self-paced learnin**g[J]. IEEE Transactions on Evolutionary Computation, 2018, 23(2): 288-302. [paper](https://ieeexplore.ieee.org/abstract/document/8396286/)
1. Sinha S, Garg A, Larochelle H. **Curriculum by smoothing**[J]. arXiv preprint arXiv:2003.01367, 2020. [paper](https://arxiv.org/pdf/2003.01367.pdf)
1. Gong C, Tao D, Maybank S J, et al. **Multi-modal curriculum learning for semi-supervised image classification**[J]. IEEE Transactions on Image Processing, 2016, 25(7): 3249-3260. [paper](https://www.dcs.bbk.ac.uk/~SJMAYBANK/MultiModal.pdf)
1. Zhou T, Bilmes J. **Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity**[C]//International Conference on Learning Representations. 2018. [paper](https://openreview.net/pdf?id=BywyFQlAW)
1. Choi J, Jeong M, Kim T, et al. **Pseudo-labeling curriculum for unsupervised domain adaptation**[J]. arXiv preprint arXiv:1908.00262, 2019. [paper](https://arxiv.org/pdf/1908.00262.pdf)
1. Huang Y, Du J. **Self-attention enhanced CNNs and collaborative curriculum learning for distantly supervised relation extraction**[C]//Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 2019: 389-398. [paper](https://aclanthology.org/D19-1037.pdf)
1. Wang X, Chen Y, Zhu W. **A survey on curriculum learning**[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [paper](https://arxiv.org/abs/2010.13166)
1. Fan Y, Tian F, Qin T, et al. **Learning to Teach[C]//International Conference on Learning Representations**. 2018. [paper](https://www.researchgate.net/profile/Yang-Fan-97/publication/325076365_Learning_to_Teach/links/5e99a97fa6fdcca78920484f/Learning-to-Teach.pdf)
1. Jiang L, Meng D, Yu S I, et al. **Self-paced learning with diversity**[J]. Advances in Neural Information Processing Systems, 2014, 27: 2078-2086. [paper](http://papers.neurips.cc/paper/5568-self-paced-learning-with-diversity.pdf)
1. Platanios E A, Stretcu O, Neubig G, et al. **Competence-based Curriculum Learning for Neural Machine Translation**[C]//Proceedings of NAACL-HLT. 2019: 1162-1172. [paper](http://aclanthology.lst.uni-saarland.de/N19-1119.pdf)
1. Bousquet O, Boucheron S, Lugosi G. **Introduction to statistical learning theory**[C]//Summer school on machine learning. Springer, Berlin, Heidelberg, 2003: 169-207. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.2036&rep=rep1&type=pdf)
1. Maurer A, Pontil M. **Empirical Bernstein bounds and sample variance penalization**[J]. arXiv preprint arXiv:0907.3740, 2009. [paper](https://arxiv.org/pdf/0907.3740.pdf)
1. He K, Chen X, Xie S, et al. **Masked autoencoders are scalable vision learners**[J]. arXiv preprint arXiv:2111.06377, 2021. [paper](https://arxiv.org/pdf/2111.06377.pdf)
1. Shalev-Shwartz S, Ben-David S. **Understanding machine learning: From theory to algorithms**[M]. Cambridge university press, 2014. [book](http://103.47.12.35/bitstream/handle/1/1069/understanding-machine-learning-theory-algorithms.pdf)
1. Duin R P W. **Small sample size generalization**[C]//Proceedings of the Scandinavian Conference on Image Analysis. PROCEEDINGS PUBLISHED BY VARIOUS PUBLISHERS, 1995, 2: 957-964. [paper](http://homepage.tudelft.nl/a9p19/papers/scia_95.sssize.pdf)
1. Loog M, Duin R P W. **The dipping phenomenon**[C]//Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR). Springer, Berlin, Heidelberg, 2012: 310-317. [paper](https://semisupervised-learning.compute.dtu.dk/wp-content/uploads/2016/08/dipping.pdf)
1. Hoi S C H, Sahoo D, Lu J, et al. **Online learning: A comprehensive survey**[J]. Neurocomputing, 2021, 459: 249-289. [paper](https://sci-hub.mksa.top/https://doi.org/10.1016/j.neucom.2021.04.112)
1. Delange M, Aljundi R, Masana M, et al. **A continual learning survey: Defying forgetting in classification tasks**[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [paper](https://homes.esat.kuleuven.be/~konijn/publications/2020/DeLange2.pdf)
1. Huisman M, Van Rijn J N, Plaat A. **A survey of deep meta-learning**[J]. Artificial Intelligence Review, 2021, 54(6): 4483-4541. [paper](https://ada.liacs.nl/papers/HuiEtAl21.pdf)
1. Tan C, Sun F, Kong T, et al. **A survey on deep transfer learning**[C]//International conference on artificial neural networks. Springer, Cham, 2018: 270-279. [paper](https://link.springer.com/content/pdf/10.1007/978-3-030-01424-7.pdf)
1. Arulkumaran K, Deisenroth M P, Brundage M, et al. **Deep reinforcement learning: A brief survey**[J]. IEEE Signal Processing Magazine, 2017, 34(6): 26-38. [paper](http://web.khu.ac.kr/~tskim/PatternClass%20LEc%20Note%2026-3%20DRL%20IEEE%20Magazine.pdf)
1. LeCun Y, Bottou L, Bengio Y, et al. **Gradient-based learning applied to document recognition**[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324. [paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
1. Simonyan K, Zisserman A. **Very deep convolutional networks for large-scale image recognition**[J]. arXiv preprint arXiv:1409.1556, 2014. [paper](https://cdn-sv1.deepsense.ai/wp-content/uploads/2017/08/1409.1556-3.pdf)
1. He K, Zhang X, Ren S, et al. **Deep residual learning for image recognition**[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778. [paper](https://www.deeplearningitalia.com/wp-content/uploads/2017/12/Dropbox_Deep-Residual-Learning-for-Image-Recognition.pdf)
1. Koolen W M, Grünwald P, Van Erven T. **Combining adversarial guarantees and stochastic fast rates in online learning**[J]. Advances in Neural Information Processing Systems, 2016, 29. [paper](https://www.researchgate.net/profile/Peter-Gruenwald/publication/303409477_Combining_Adversarial_Guarantees_and_Stochastic_Fast_Rates_in_Online_Learning/links/591dea30a6fdcc233fcead50/Combining-Adversarial-Guarantees-and-Stochastic-Fast-Rates-in-Online-Learning.pdf)
1. Vaswani A, Shazeer N, Parmar N, et al. **Attention is all you need**[J]. Advances in neural information processing systems, 2017, 30. [paper](https://static.aminer.cn/upload/pdf/868/768/1297/599c7987601a182cd2648373.pdf)
1. Devlin J, Chang M W, Lee K, et al. **Bert: Pre-training of deep bidirectional transformers for language understanding**[J]. arXiv preprint arXiv:1810.04805, 2018. [paper](https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ)
1. Davis S E, Lasko T A, Chen G, et al. **Calibration drift in regression and machine learning models for acute kidney injury**[J]. Journal of the American Medical Informatics Association, 2017, 24(6): 1052-1061. [paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6080675/)
1. Darlow L N, Crowley E J, Antoniou A, et al. **Cinic-10 is not imagenet or cifar-10**[J]. arXiv preprint arXiv:1810.03505, 2018. [paper](https://arxiv.org/pdf/1810.03505.pdf)
1. Belkin M, Hsu D, Ma S, et al. **Reconciling modern machine-learning practice and the classical bias–variance trade-off**[J]. Proceedings of the National Academy of Sciences, 2019, 116(32): 15849-15854. [paper](https://arxiv.org/pdf/1812.11118.pdf)