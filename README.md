# Computational Learning Theory

## [Important papers](#content)
1. Kearns M J, Vazirani U. **An introduction to computational learning theory**[M]. MIT press, 1994. [book](https://books.google.com/books?hl=zh-CN&lr=&id=vCA01wY6iywC&oi=fnd&pg=PR11&ots=p3M5eXutCA&sig=IyM8SW_y0WxvrLkwNZDv6GeOSUc)
1. 周志华. **机器学习**. 清华大学出版社, 2016. [book](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm)
1. Shalev-Shwartz S, Ben-David S. **Understanding machine learning: From theory to algorithms**[M]. Cambridge university press, 2014. [paper](http://103.47.12.35/bitstream/handle/1/1069/understanding-machine-learning-theory-algorithms.pdf?sequence=1&isAllowed=y)
1. Viering T, Mey A, Loog M. **Open problem: Monotonicity of learning**[C]//Conference on Learning Theory. PMLR, 2019: 3198-3201. [paper](http://proceedings.mlr.press/v99/viering19a/viering19a.pdf)
1. Loog M, Viering T, Mey A. **Minimizers of the empirical risk and risk monotonicity**[J]. Advances in Neural Information Processing Systems, 2019, 32: 7478-7487. [paper](https://arxiv.org/pdf/1907.05476.pdf)
1. Savage L J. **The Foundations of Statistics**[M]. Courier Corporation, 2012. [book](https://books.google.com/books?hl=zh-CN&lr=&id=N_bBAgAAQBAJ&oi=fnd&pg=PA1&ots=8uoDYHkm-n&sig=A_hAwDj_75zC4jIBsXviGyicmR4)
1. Good I J. **On the principle of total evidence**[J]. The British Journal for the Philosophy of Science, 1967, 17(4): 319-321. [paper](https://www.jstor.org/stable/pdf/686773.pdf?casa_token=D7XZRowDd30AAAAA:wmGJvHj1AvbGc76It-oP3wPeKI6n1hvqYVhJQS6OPHqHAAd48Dsv2UvZ5a3I_xw5H3zrC3gqRx73DtT9PY0P_QNTTW0vX-h__Dt1WZDa8GKdHaEWv3Wkrg)
1. Grunwald P D, Halpern J Y. **When ignorance is bliss**[J]. arXiv preprint arXiv:1407.7188, 2014. [paper](https://www.researchgate.net/profile/Peter-Gruenwald/publication/221404811_When_Ignorance_is_Bliss/links/546378a60cf2c0c6aec4c031/When-Ignorance-is-Bliss.pdf)
1. Viering T, Loog M. **The Shape of Learning Curves: a Review**[J]. arXiv preprint arXiv:2103.10948, 2021. [paper](https://arxiv.org/pdf/2103.10948.pdf)
1. Viering T J, Mey A, Loog M. **Making learners (more) monotone**[C]//International Symposium on Intelligent Data Analysis. Springer, Cham, 2020: 535-547. [paper](https://www.researchgate.net/profile/Tom-Viering/publication/337531868_Making_Learners_More_Monotone/links/5f4e07b9a6fdcc14c504d66c/Making-Learners-More-Monotone.pdf)
1. Mhammedi Z, Husain H. **Risk-Monotonicity in Statistical Learning**[J]. arXiv preprint arXiv:2011.14126, 2020. [paper](https://arxiv.org/pdf/2011.14126.pdf)
1. Soviany P, Ionescu R T, Rota P, et al. **Curriculum learning: A survey**[J]. arXiv preprint arXiv:2101.10382, 2021. [paper](https://arxiv.org/pdf/2101.10382.pdf)
1. Bengio Y, Louradour J, Collobert R, et al. **Curriculum learning**[C]//Proceedings of the 26th annual international conference on machine learning. 2009: 41-48. [paper](http://www.thespermwhale.com/jaseweston/papers/curriculum.pdf)
1. Kumar M P, Packer B, Koller D. **Self-Paced Learning for Latent Variable Models**[C]//NIPS. 2010, 1: 2. [paper](http://papers.neurips.cc/paper/3923-self-paced-learning-for-latent-variable-models.pdf)
1. Jiang L, Meng D, Zhao Q, et al. **Self-paced curriculum learning**[C]//Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015. [paper](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewFile/%0B9750/9929)
1. Kim T H, Choi J. Screenernet: **Learning self-paced curriculum for deep neural networks**[J]. arXiv preprint arXiv:1801.00904, 2018. [paper](https://arxiv.org/pdf/1801.00904.pdf)
1. Morerio P, Cavazza J, Volpi R, et al. **Curriculum dropout**[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 3544-3552. [paper](http://www.vision.jhu.edu/assets/MorerioICCV17.pdf)
1. Krogh A, Vedelsby J. **Neural network ensembles, cross validation, and active learning**[J]. Advances in neural information processing systems, 1995, 7: 231-238. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.52.9672&rep=rep1&type=pdf)
1. Gong M, Li H, Meng D, et al. **Decomposition-based evolutionary multiobjective optimization to self-paced learnin**g[J]. IEEE Transactions on Evolutionary Computation, 2018, 23(2): 288-302. [paper](https://ieeexplore.ieee.org/abstract/document/8396286/)
1. Sinha S, Garg A, Larochelle H. **Curriculum by smoothing**[J]. arXiv preprint arXiv:2003.01367, 2020. [paper](https://arxiv.org/pdf/2003.01367.pdf)
1. Gong C, Tao D, Maybank S J, et al. **Multi-modal curriculum learning for semi-supervised image classification**[J]. IEEE Transactions on Image Processing, 2016, 25(7): 3249-3260. [paper](https://www.dcs.bbk.ac.uk/~SJMAYBANK/MultiModal.pdf)
1. Zhou T, Bilmes J. **Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity**[C]//International Conference on Learning Representations. 2018. [paper](https://openreview.net/pdf?id=BywyFQlAW)
1. Choi J, Jeong M, Kim T, et al. **Pseudo-labeling curriculum for unsupervised domain adaptation**[J]. arXiv preprint arXiv:1908.00262, 2019. [paper](https://arxiv.org/pdf/1908.00262.pdf)
1. Huang Y, Du J. **Self-attention enhanced CNNs and collaborative curriculum learning for distantly supervised relation extraction**[C]//Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 2019: 389-398. [paper](https://aclanthology.org/D19-1037.pdf)
1. Wang X, Chen Y, Zhu W. **A survey on curriculum learning**[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. [paper](https://arxiv.org/abs/2010.13166)
1. Fan Y, Tian F, Qin T, et al. **Learning to Teach[C]//International Conference on Learning Representations**. 2018. [paper](https://www.researchgate.net/profile/Yang-Fan-97/publication/325076365_Learning_to_Teach/links/5e99a97fa6fdcca78920484f/Learning-to-Teach.pdf)
1. Jiang L, Meng D, Yu S I, et al. **Self-paced learning with diversity**[J]. Advances in Neural Information Processing Systems, 2014, 27: 2078-2086. [paper](http://papers.neurips.cc/paper/5568-self-paced-learning-with-diversity.pdf)
1. Platanios E A, Stretcu O, Neubig G, et al. **Competence-based Curriculum Learning for Neural Machine Translation**[C]//Proceedings of NAACL-HLT. 2019: 1162-1172. [paper](http://aclanthology.lst.uni-saarland.de/N19-1119.pdf)
1. Bousquet O, Boucheron S, Lugosi G. **Introduction to statistical learning theory**[C]//Summer school on machine learning. Springer, Berlin, Heidelberg, 2003: 169-207. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.2036&rep=rep1&type=pdf)
1. Maurer A, Pontil M. **Empirical Bernstein bounds and sample variance penalization**[J]. arXiv preprint arXiv:0907.3740, 2009. [paper](https://arxiv.org/pdf/0907.3740.pdf)
1. He K, Chen X, Xie S, et al. **Masked autoencoders are scalable vision learners**[J]. arXiv preprint arXiv:2111.06377, 2021. [paper](https://arxiv.org/pdf/2111.06377.pdf)
1. Shalev-Shwartz S, Ben-David S. **Understanding machine learning: From theory to algorithms**[M]. Cambridge university press, 2014. [book](http://103.47.12.35/bitstream/handle/1/1069/understanding-machine-learning-theory-algorithms.pdf)











